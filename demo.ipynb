{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from src.tensor import Tensor\n",
    "from src.activation_function import Linear, ReLU, Sigmoid, Softmax\n",
    "from src.loss_function import MeanSquaredError\n",
    "from src.layer import Dense\n",
    "from src.model import FFNN\n",
    "from src.weight_initializer import GlorotUniformInitializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tensor Test Case**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value: [1. 2.], Gradient: [0. 0.], Op: None\n",
      "Value: [3. 4.], Gradient: [0. 0.], Op: None\n",
      "Value: [4. 6.], Gradient: [0. 0.], Op: +\n",
      "Value: [-2. -2.], Gradient: [0. 0.], Op: +\n",
      "Value: [3. 8.], Gradient: [0. 0.], Op: *\n",
      "Value: [0.33333333 0.5       ], Gradient: [0. 0.], Op: *\n"
     ]
    }
   ],
   "source": [
    "# Basic operations\n",
    "a = Tensor(np.array([1,2]))\n",
    "b = Tensor(np.array([3,4]))\n",
    "c = a + b\n",
    "d = a - b\n",
    "e = a * b\n",
    "f = a / b\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "print(d)\n",
    "print(e)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value: [1. 2.], Gradient: [0. 0.], Op: None\n",
      "Value: [1. 2.], Gradient: [0. 0.], Op: Linear\n",
      "Value: [1. 2.], Gradient: [0. 0.], Op: ReLU\n",
      "Value: [4.], Gradient: [0.], Op: MeanSquaredError\n",
      "Value: [4.], Gradient: [0.], Op: MeanSquaredError\n"
     ]
    }
   ],
   "source": [
    "# Activation function and loss function\n",
    "a = Tensor(np.array([1,2]))\n",
    "b = a.compute_activation(Linear)\n",
    "c = a.compute_activation(ReLU)\n",
    "d = b.compute_loss(Tensor(np.array([3,4])), MeanSquaredError)\n",
    "e = c.compute_loss(Tensor(np.array([3,4])), MeanSquaredError)\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "print(d)\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Before backpropagation ----------\n",
      "Value: [1. 2.], Gradient: [0. 0.], Op: None\n",
      "Value: [3. 4.], Gradient: [0. 0.], Op: None\n",
      "Value: [4. 6.], Gradient: [0. 0.], Op: +\n",
      "Value: [-2. -2.], Gradient: [0. 0.], Op: +\n",
      "Value: [ -8. -12.], Gradient: [0. 0.], Op: *\n",
      "Value: [ -8. -12.], Gradient: [0. 0.], Op: Linear\n",
      "Value: [0. 0.], Gradient: [0. 0.], Op: ReLU\n",
      "Value: [125.], Gradient: [0.], Op: MeanSquaredError\n",
      "Value: [1.], Gradient: [0.], Op: MeanSquaredError\n",
      "\n",
      "---------- After backpropagation ----------\n",
      "Value: [1. 2.], Gradient: [-108. -312.], Op: None\n",
      "Value: [3. 4.], Gradient: [108. 156.], Op: None\n",
      "Value: [4. 6.], Gradient: [ 72. 104.], Op: +\n",
      "Value: [-2. -2.], Gradient: [-144. -312.], Op: +\n",
      "Value: [ -8. -12.], Gradient: [-18. -26.], Op: *\n",
      "Value: [ -8. -12.], Gradient: [-18. -26.], Op: Linear\n",
      "Value: [0. 0.], Gradient: [-2. -2.], Op: ReLU\n",
      "Value: [125.], Gradient: [1.], Op: MeanSquaredError\n",
      "Value: [1.], Gradient: [1.], Op: MeanSquaredError\n"
     ]
    }
   ],
   "source": [
    "# Automatic differentiation\n",
    "a = Tensor(np.array([1,2]))\n",
    "b = Tensor(np.array([3,4]))\n",
    "c = a + b\n",
    "d = a - b\n",
    "e = c * d\n",
    "f = e.compute_activation(Linear)\n",
    "g = e.compute_activation(ReLU)\n",
    "h = f.compute_loss(np.array([1,1]), MeanSquaredError)\n",
    "i = g.compute_loss(np.array([1,1]), MeanSquaredError)\n",
    "\n",
    "print(\"---------- Before backpropagation ----------\")\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "print(d)\n",
    "print(e)\n",
    "print(f)\n",
    "print(g)\n",
    "print(h)\n",
    "print(i)\n",
    "\n",
    "h.backward()\n",
    "i.backward()\n",
    "\n",
    "print(\"\\n---------- After backpropagation ----------\")\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "print(d)\n",
    "print(e)\n",
    "print(f)\n",
    "print(g)\n",
    "print(h)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Before backpropagation ----------\n",
      "Value: [2. 3. 4.], Gradient: [0. 0. 0.], Op: None, Type: weight\n",
      "Value: [3. 4. 5.], Gradient: [0. 0. 0.], Op: None, Type: weight\n",
      "Value: [ 2.  6. 12.], Gradient: [0. 0. 0.], Op: *\n",
      "Value: [ 3.  8. 15.], Gradient: [0. 0. 0.], Op: *\n",
      "Value: [20.], Gradient: [0.], Op: sum\n",
      "Value: [26.], Gradient: [0.], Op: sum\n",
      "Value: [20.], Gradient: [0.], Op: ReLU\n",
      "Value: [26.], Gradient: [0.], Op: ReLU\n",
      "Value: [20. 26.], Gradient: [0. 0.], Op: concat\n",
      "Value: [80.], Gradient: [0.], Op: MeanSquaredError\n",
      "\n",
      "---------- After backpropagation ----------\n",
      "Value: [2. 3. 4.], Gradient: [ 8. 16. 24.], Op: None, Type: weight\n",
      "Value: [3. 4. 5.], Gradient: [24. 48. 72.], Op: None, Type: weight\n",
      "Value: [ 2.  6. 12.], Gradient: [8. 8. 8.], Op: *\n",
      "Value: [ 3.  8. 15.], Gradient: [24. 24. 24.], Op: *\n",
      "Value: [20.], Gradient: [8.], Op: sum\n",
      "Value: [26.], Gradient: [24.], Op: sum\n",
      "Value: [20.], Gradient: [8.], Op: ReLU\n",
      "Value: [26.], Gradient: [24.], Op: ReLU\n",
      "Value: [20. 26.], Gradient: [ 8. 24.], Op: concat\n",
      "Value: [80.], Gradient: [1.], Op: MeanSquaredError\n"
     ]
    }
   ],
   "source": [
    "## Simulation of one layer with two neurons (h1 and h2)\n",
    "\n",
    "# Initial values\n",
    "x = Tensor(np.array([1, 2, 3]), tensor_type=\"input\")            # input, x[0] is always 1\n",
    "y = np.array([16, 14])                                          # correct class / y_true\n",
    "wh1 = Tensor(np.array([2, 3, 4]), tensor_type=\"weight\")         # weights of neuron h1, wh1[0] = b1 (bias)\n",
    "wh2 = Tensor(np.array([3, 4, 5]), tensor_type=\"weight\")         # weights of neuron h2, wh2[0] = b2 (bias)\n",
    "\n",
    "# Calculate net\n",
    "wh1_x = wh1 * x\n",
    "wh2_x = wh2 * x\n",
    "net1 = wh1_x.sum()\n",
    "net2 = wh2_x.sum()\n",
    "\n",
    "# Calculate output\n",
    "o1 = net1.compute_activation(ReLU)\n",
    "o2 = net2.compute_activation(ReLU)\n",
    "\n",
    "# Calculate loss\n",
    "output = o1.concat([o2])\n",
    "loss = output.compute_loss(y, MeanSquaredError)\n",
    "\n",
    "print(\"---------- Before backpropagation ----------\")\n",
    "print(wh1)\n",
    "print(wh2)\n",
    "print(wh1_x)\n",
    "print(wh2_x)\n",
    "print(net1)\n",
    "print(net2)\n",
    "print(o1)\n",
    "print(o2)\n",
    "print(output)\n",
    "print(loss)\n",
    "\n",
    "# Initiate automated differentiation\n",
    "loss.backward()\n",
    "\n",
    "print(\"\\n---------- After backpropagation ----------\")\n",
    "print(wh1)\n",
    "print(wh2)\n",
    "print(wh1_x)\n",
    "print(wh2_x)\n",
    "print(net1)\n",
    "print(net2)\n",
    "print(o1)\n",
    "print(o2)\n",
    "print(output)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ Layer 1 ============\n",
      "Value: [ 2.  6. 12.], Gradient: [16291060. 16291060. 16291060.], Op: *\n",
      "Value: [ 3.  8. 15.], Gradient: [20034510. 20034510. 20034510.], Op: *\n",
      "Value: [ 4. 10. 18.], Gradient: [23777960. 23777960. 23777960.], Op: *\n",
      "Value: [20.], Gradient: [16291060.], Op: sum\n",
      "Value: [26.], Gradient: [20034510.], Op: sum\n",
      "Value: [32.], Gradient: [23777960.], Op: sum\n",
      "Value: [20.], Gradient: [16291060.], Op: ReLU\n",
      "Value: [26.], Gradient: [20034510.], Op: ReLU\n",
      "Value: [32.], Gradient: [23777960.], Op: ReLU\n",
      "Value: [ 1. 20. 26. 32.], Gradient: [16291060. 20034510. 23777960. 27521410.], Op: concat\n",
      "\n",
      "============ Layer 2 ============\n",
      "Value: [ 1. 20. 26. 32.], Gradient: [16291060. 20034510. 23777960. 27521410.], Op: concat\n",
      "Value: [  2.  60. 104. 160.], Gradient: [485238. 485238. 485238. 485238.], Op: *\n",
      "Value: [  3.  80. 130. 192.], Gradient: [616964. 616964. 616964. 616964.], Op: *\n",
      "Value: [  4. 100. 156. 224.], Gradient: [748690. 748690. 748690. 748690.], Op: *\n",
      "Value: [  5. 120. 182. 256.], Gradient: [880416. 880416. 880416. 880416.], Op: *\n",
      "Value: [  6. 140. 208. 288.], Gradient: [1012142. 1012142. 1012142. 1012142.], Op: *\n",
      "Value: [326.], Gradient: [485238.], Op: sum\n",
      "Value: [405.], Gradient: [616964.], Op: sum\n",
      "Value: [484.], Gradient: [748690.], Op: sum\n",
      "Value: [563.], Gradient: [880416.], Op: sum\n",
      "Value: [642.], Gradient: [1012142.], Op: sum\n",
      "Value: [326.], Gradient: [485238.], Op: ReLU\n",
      "Value: [405.], Gradient: [616964.], Op: ReLU\n",
      "Value: [484.], Gradient: [748690.], Op: ReLU\n",
      "Value: [563.], Gradient: [880416.], Op: ReLU\n",
      "Value: [642.], Gradient: [1012142.], Op: ReLU\n",
      "Value: [  1. 326. 405. 484. 563. 642.], Gradient: [ 485238.  616964.  748690.  880416. 1012142. 1143868.], Op: concat\n",
      "\n",
      "============ Layer 3 ============\n",
      "Value: [  1. 326. 405. 484. 563. 642.], Gradient: [ 485238.  616964.  748690.  880416. 1012142. 1143868.], Op: concat\n",
      "Value: [2.000e+00 9.780e+02 1.620e+03 2.420e+03 3.378e+03 4.494e+03], Gradient: [25684. 25684. 25684. 25684. 25684. 25684.], Op: *\n",
      "Value: [3.000e+00 1.304e+03 2.025e+03 2.904e+03 3.941e+03 5.136e+03], Gradient: [30498. 30498. 30498. 30498. 30498. 30498.], Op: *\n",
      "Value: [4.000e+00 1.630e+03 2.430e+03 3.388e+03 4.504e+03 5.778e+03], Gradient: [35344. 35344. 35344. 35344. 35344. 35344.], Op: *\n",
      "Value: [5.000e+00 1.956e+03 2.835e+03 3.872e+03 5.067e+03 6.420e+03], Gradient: [40200. 40200. 40200. 40200. 40200. 40200.], Op: *\n",
      "Value: [12892.], Gradient: [25684.], Op: sum\n",
      "Value: [15313.], Gradient: [30498.], Op: sum\n",
      "Value: [17734.], Gradient: [35344.], Op: sum\n",
      "Value: [20155.], Gradient: [40200.], Op: sum\n",
      "Value: [12892.], Gradient: [25684.], Op: ReLU\n",
      "Value: [15313.], Gradient: [30498.], Op: ReLU\n",
      "Value: [17734.], Gradient: [35344.], Op: ReLU\n",
      "Value: [20155.], Gradient: [40200.], Op: ReLU\n",
      "Value: [12892. 15313. 17734. 20155.], Gradient: [25684. 30498. 35344. 40200.], Op: concat\n",
      "Value: [2.78439637e+08], Gradient: [1.], Op: MeanSquaredError\n"
     ]
    }
   ],
   "source": [
    "## Simulation of 3-layered (excluding input layer) network with n = [3, 5, 4] number of neurons\n",
    "\n",
    "# Initial values\n",
    "x = Tensor(np.array([1, 2, 3]), tensor_type=\"input\")            \n",
    "y = np.array([50, 64, 62, 55])                                          \n",
    "wh1 = Tensor(np.array([2, 3, 4]), tensor_type=\"weight\")\n",
    "wh2 = Tensor(np.array([3, 4, 5]), tensor_type=\"weight\")\n",
    "wh3 = Tensor(np.array([4, 5, 6]), tensor_type=\"weight\")\n",
    "wh4 = Tensor(np.array([2, 3, 4, 5]), tensor_type=\"weight\")\n",
    "wh5 = Tensor(np.array([3, 4, 5, 6]), tensor_type=\"weight\")\n",
    "wh6 = Tensor(np.array([4, 5, 6, 7]), tensor_type=\"weight\")\n",
    "wh7 = Tensor(np.array([5, 6, 7, 8]), tensor_type=\"weight\")\n",
    "wh8 = Tensor(np.array([6, 7, 8, 9]), tensor_type=\"weight\")\n",
    "wh9 = Tensor(np.array([2, 3, 4, 5, 6, 7]), tensor_type=\"weight\")\n",
    "wh10 = Tensor(np.array([3, 4, 5, 6, 7, 8]), tensor_type=\"weight\")\n",
    "wh11 = Tensor(np.array([4, 5, 6, 7, 8, 9]), tensor_type=\"weight\")\n",
    "wh12 = Tensor(np.array([5, 6, 7, 8, 9, 10]), tensor_type=\"weight\")\n",
    "\n",
    "\n",
    "# Layer 1\n",
    "wh1_x = wh1 * x\n",
    "wh2_x = wh2 * x\n",
    "wh3_x = wh3 * x\n",
    "net1 = wh1_x.sum()\n",
    "net2 = wh2_x.sum()\n",
    "net3 = wh3_x.sum()\n",
    "o1 = net1.compute_activation(ReLU)\n",
    "o2 = net2.compute_activation(ReLU)\n",
    "o3 = net3.compute_activation(ReLU)\n",
    "output_l1 = o1.concat([o2, o3])\n",
    "\n",
    "# Layer 2\n",
    "input_l2 = output_l1.add_x0()\n",
    "wh4_l2 = wh4 * input_l2\n",
    "wh5_l2 = wh5 * input_l2\n",
    "wh6_l2 = wh6 * input_l2\n",
    "wh7_l2 = wh7 * input_l2\n",
    "wh8_l2 = wh8 * input_l2\n",
    "net4 = wh4_l2.sum()\n",
    "net5 = wh5_l2.sum()\n",
    "net6 = wh6_l2.sum()\n",
    "net7 = wh7_l2.sum()\n",
    "net8 = wh8_l2.sum()\n",
    "o4 = net4.compute_activation(ReLU)\n",
    "o5 = net5.compute_activation(ReLU)\n",
    "o6 = net6.compute_activation(ReLU)\n",
    "o7 = net7.compute_activation(ReLU)\n",
    "o8 = net8.compute_activation(ReLU)\n",
    "output_l2 = o4.concat([o5, o6, o7, o8])\n",
    "\n",
    "# Layer 3\n",
    "input_l3 = output_l2.add_x0()\n",
    "wh9_l3 = wh9 * input_l3\n",
    "wh10_l3 = wh10 * input_l3\n",
    "wh11_l3 = wh11 * input_l3\n",
    "wh12_l3 = wh12 * input_l3\n",
    "net9 = wh9_l3.sum()\n",
    "net10 = wh10_l3.sum()\n",
    "net11 = wh11_l3.sum()\n",
    "net12 = wh12_l3.sum()\n",
    "o9 = net9.compute_activation(ReLU)\n",
    "o10 = net10.compute_activation(ReLU)\n",
    "o11 = net11.compute_activation(ReLU)\n",
    "o12 = net12.compute_activation(ReLU)\n",
    "output_l3 = o9.concat([o10, o11, o12])\n",
    "loss = output_l3.compute_loss(y, MeanSquaredError)\n",
    "\n",
    "\n",
    "# Backpropagation\n",
    "loss.backward()\n",
    "\n",
    "print(\"============ Layer 1 ============\")\n",
    "print(wh1_x)\n",
    "print(wh2_x)\n",
    "print(wh3_x)\n",
    "print(net1)\n",
    "print(net2)\n",
    "print(net3)\n",
    "print(o1)\n",
    "print(o2)\n",
    "print(o3)\n",
    "print(output_l1)\n",
    "print(\"\\n============ Layer 2 ============\")\n",
    "print(input_l2)\n",
    "print(wh4_l2)\n",
    "print(wh5_l2)\n",
    "print(wh6_l2)\n",
    "print(wh7_l2)\n",
    "print(wh8_l2)\n",
    "print(net4)\n",
    "print(net5)\n",
    "print(net6)\n",
    "print(net7)\n",
    "print(net8)\n",
    "print(o4)\n",
    "print(o5)\n",
    "print(o6)\n",
    "print(o7)\n",
    "print(o8)\n",
    "print(output_l2)\n",
    "print(\"\\n============ Layer 3 ============\")\n",
    "print(input_l3)\n",
    "print(wh9_l3)\n",
    "print(wh10_l3)\n",
    "print(wh11_l3)\n",
    "print(wh12_l3)\n",
    "print(net9)\n",
    "print(net10)\n",
    "print(net11)\n",
    "print(net12)\n",
    "print(o9)\n",
    "print(o10)\n",
    "print(o11)\n",
    "print(o12)\n",
    "print(output_l3)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Layer Test Case**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred: Value: [ 0.50284256 -1.18364441 -2.0961996   2.3863882   6.43805638], Gradient: [0. 0. 0. 0. 0.], Op: concat\n",
      "Loss: Value: [13.84302125], Gradient: [0.], Op: MeanSquaredError\n",
      "\n",
      "---------- Before backpropagation ----------\n",
      "Weights: [Value: [-0.2589774  -0.61380347  0.08420048  0.43420637], Gradient: [0. 0. 0. 0.], Op: None, Type: weight, Value: [ 0.35914784 -0.32883166  0.25210794 -0.41036319], Gradient: [0. 0. 0. 0.], Op: None, Type: weight, Value: [-0.23642688  0.19655381 -0.27344621 -0.35813542], Gradient: [0. 0. 0. 0.], Op: None, Type: weight, Value: [ 0.64991335 -0.55274697  0.19454905  0.56458041], Gradient: [0. 0. 0. 0.], Op: None, Type: weight, Value: [-0.15256221  0.80292237  0.70136793  0.72016752], Gradient: [0. 0. 0. 0.], Op: None, Type: weight]\n",
      "\n",
      "---------- After backpropagation ----------\n",
      "Weights: [Value: [-0.2589774  -0.61380347  0.08420048  0.43420637], Gradient: [ -8.99431489 -17.98862978 -26.98294467 -35.97725955], Op: None, Type: weight, Value: [ 0.35914784 -0.32883166  0.25210794 -0.41036319], Gradient: [-2.36728882 -4.73457765 -7.10186647 -9.46915529], Op: None, Type: weight, Value: [-0.23642688  0.19655381 -0.27344621 -0.35813542], Gradient: [-10.1923992  -20.3847984  -30.57719759 -40.76959679], Op: None, Type: weight, Value: [ 0.64991335 -0.55274697  0.19454905  0.56458041], Gradient: [ 2.77277641  5.54555282  8.31832922 11.09110563], Op: None, Type: weight, Value: [-0.15256221  0.80292237  0.70136793  0.72016752], Gradient: [ 8.87611277 17.75222554 26.62833831 35.50445108], Op: None, Type: weight]\n"
     ]
    }
   ],
   "source": [
    "## Simulation of one-layered network with n = 5 number of neurons\n",
    "\n",
    "x = Tensor(np.array([1, 2, 3, 4]))      # input, x[0] is always 1\n",
    "y = np.array([5, 0, 3, 1, 2])           # correct class / y_true\n",
    "\n",
    "layer = Dense(neuron_size=5, activation=\"linear\", kernel_initializer=\"glorot_uniform\", input_size=3) # output layer with 5 neurons\n",
    "\n",
    "# Forwardpropagation\n",
    "y_pred = layer.forward(x)\n",
    "loss = y_pred.compute_loss(y, MeanSquaredError)\n",
    "print(\"y_pred:\", y_pred)\n",
    "print(\"Loss:\", loss)\n",
    "\n",
    "print(\"\\n---------- Before backpropagation ----------\")\n",
    "print('Weights:', layer.weights)\n",
    "\n",
    "# Backpropagation\n",
    "loss.backward()\n",
    "print(\"\\n---------- After backpropagation ----------\")\n",
    "print(\"Weights:\", layer.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **FFNN Test Case**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred: [-3.07647716 -3.3539429  -3.07761871  0.09343685 -0.26891757]\n",
      "\n",
      "---------- Before backpropagation ----------\n",
      "[Value: [ 0.11884102  0.1050885  -0.10286608 -0.77422423], Gradient: [0. 0. 0. 0.], Op: None, Type: weight, Value: [-0.13889216  0.15994382 -0.12585664 -0.78934212], Gradient: [0. 0. 0. 0.], Op: None, Type: weight, Value: [ 0.21023145 -0.21134297 -0.2945223  -0.49539933], Gradient: [0. 0. 0. 0.], Op: None, Type: weight, Value: [-0.64307675 -0.81527004  0.71662503  0.05429465], Gradient: [0. 0. 0. 0.], Op: None, Type: weight, Value: [ 0.10329213 -0.32840355 -0.18440626  0.20945404], Gradient: [0. 0. 0. 0.], Op: None, Type: weight]\n",
      "\n",
      "---------- After backpropagation ----------\n",
      "[Value: [ 0.11884102  0.1050885  -0.10286608 -0.77422423], Gradient: [-16.15295432 -32.30590865 -48.45886297 -64.6118173 ], Op: None, Type: weight, Value: [-0.13889216  0.15994382 -0.12585664 -0.78934212], Gradient: [ -6.70788581 -13.41577162 -20.12365742 -26.83154323], Op: None, Type: weight, Value: [ 0.21023145 -0.21134297 -0.2945223  -0.49539933], Gradient: [-12.15523742 -24.31047484 -36.46571227 -48.62094969], Op: None, Type: weight, Value: [-0.64307675 -0.81527004  0.71662503  0.05429465], Gradient: [-1.8131263  -3.6262526  -5.43937889 -7.25250519], Op: None, Type: weight, Value: [ 0.10329213 -0.32840355 -0.18440626  0.20945404], Gradient: [ -4.53783515  -9.07567029 -13.61350544 -18.15134059], Op: None, Type: weight]\n",
      "gradient: [Value: [-3.07647716], Gradient: [-16.15295432], Op: sum, Value: [-3.3539429], Gradient: [-6.70788581], Op: sum, Value: [-3.07761871], Gradient: [-12.15523742], Op: sum, Value: [0.09343685], Gradient: [-1.8131263], Op: sum, Value: [-0.26891757], Gradient: [-4.53783515], Op: sum]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Simulation of one-layered network with n = 5 number of neurons\n",
    "\n",
    "x = Tensor(np.array([2, 3, 4]))      \n",
    "y = np.array([5, 0, 3, 1, 2])           \n",
    "\n",
    "# Build the FFNN model\n",
    "model = FFNN([\n",
    "    Dense(neuron_size=5, activation=\"linear\", kernel_initializer=\"glorot_uniform\", input_size=3)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"sgd\", loss=\"mean_squared_error\")\n",
    "\n",
    "# Predict output (without fitting)\n",
    "y_pred = model.forward(x)\n",
    "print(\"y_pred:\", y_pred)\n",
    "print(\"\\n---------- Before backpropagation ----------\")\n",
    "for layer in model.layers:\n",
    "    print(layer.weights)\n",
    "\n",
    "# Initiate backwardpropagation\n",
    "model.backward(y)\n",
    "print(\"\\n---------- After backpropagation ----------\")\n",
    "for layer in model.layers:\n",
    "    print(layer.weights)\n",
    "    print(\"gradient:\", layer.gradients)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_linearly_separable_data(num_samples, num_features=2, weight=None, bias=1, random_seed=None):\n",
    "    if num_features < 2:\n",
    "        raise ValueError(\"num_features must be at least 2 for meaningful separation.\")\n",
    "    \n",
    "    if random_seed is not None:\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "    X = np.random.rand(num_samples, num_features) * 20\n",
    "\n",
    "    if weight is None:\n",
    "        weight = np.random.uniform(-5, 5, size=(num_features - 1))\n",
    "\n",
    "    decision_boundary = np.dot(X[:, :-1], weight) + X[:, -1]\n",
    "    y = np.array(decision_boundary > (bias + 10), dtype=int).reshape(-1, 1)\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation Loss: 0.3712534901616879\n",
      "Epoch 2, Validation Loss: 0.3068719156906661\n",
      "Epoch 3, Validation Loss: 0.25092093220541273\n",
      "Epoch 4, Validation Loss: 0.20485521274337612\n",
      "Epoch 5, Validation Loss: 0.16825929391325575\n",
      "Epoch 6, Validation Loss: 0.13970715877104514\n",
      "Epoch 7, Validation Loss: 0.11752963910877186\n",
      "Epoch 8, Validation Loss: 0.10022601665396334\n",
      "Epoch 9, Validation Loss: 0.08659515094928029\n",
      "Epoch 10, Validation Loss: 0.07572741186583176\n",
      "Epoch 11, Validation Loss: 0.0669504221683437\n",
      "Epoch 12, Validation Loss: 0.05977078038505271\n",
      "Epoch 13, Validation Loss: 0.05382565665462866\n",
      "Epoch 14, Validation Loss: 0.048846303470776266\n",
      "Epoch 15, Validation Loss: 0.044631681883829266\n",
      "Epoch 16, Validation Loss: 0.041029721798979124\n",
      "Epoch 17, Validation Loss: 0.03792406244753296\n",
      "Epoch 18, Validation Loss: 0.03522464398835805\n",
      "Epoch 19, Validation Loss: 0.03286098683683799\n",
      "Epoch 20, Validation Loss: 0.030777348431522546\n",
      "Epoch 21, Validation Loss: 0.028929198328951897\n",
      "Epoch 22, Validation Loss: 0.027280626172536664\n",
      "Epoch 23, Validation Loss: 0.025802415762803312\n",
      "Epoch 24, Validation Loss: 0.024470599366032977\n",
      "Epoch 25, Validation Loss: 0.023265361704275952\n",
      "Epoch 26, Validation Loss: 0.022170201088276855\n",
      "Epoch 27, Validation Loss: 0.021171281483493065\n",
      "Epoch 28, Validation Loss: 0.020256927685811613\n",
      "Epoch 29, Validation Loss: 0.01941722873806684\n",
      "Epoch 30, Validation Loss: 0.0186437239290094\n",
      "Epoch 31, Validation Loss: 0.017929152324719134\n",
      "Epoch 32, Validation Loss: 0.017267251566419863\n",
      "Epoch 33, Validation Loss: 0.016652595162321704\n",
      "Epoch 34, Validation Loss: 0.01608046007424815\n",
      "Epoch 35, Validation Loss: 0.015546718310554574\n",
      "Epoch 36, Validation Loss: 0.015047747666976011\n",
      "Epoch 37, Validation Loss: 0.014580357835606505\n",
      "Epoch 38, Validation Loss: 0.014141728921588418\n",
      "Epoch 39, Validation Loss: 0.013729360033941344\n",
      "Epoch 40, Validation Loss: 0.013341026099767317\n",
      "Epoch 41, Validation Loss: 0.012974741425356364\n",
      "Epoch 42, Validation Loss: 0.012628728819699753\n",
      "Epoch 43, Validation Loss: 0.012301393325082406\n",
      "Epoch 44, Validation Loss: 0.011991299780340721\n",
      "Epoch 45, Validation Loss: 0.011697153586001994\n",
      "Epoch 46, Validation Loss: 0.011417784155168942\n",
      "Epoch 47, Validation Loss: 0.011152130625993522\n",
      "Epoch 48, Validation Loss: 0.010899229485739207\n",
      "Epoch 49, Validation Loss: 0.010658203816484522\n",
      "Epoch 50, Validation Loss: 0.010428253921361189\n",
      "[array([0.89532742]), array([0.88670845]), array([0.89539793]), array([0.88276928]), array([0.88868389]), array([0.90528676])]\n"
     ]
    }
   ],
   "source": [
    "## Simulation of three-layered network with n = [5, 7, 3] number of neurons, with fiting phase\n",
    "\n",
    "# Generate dataset\n",
    "X, y = generate_linearly_separable_data(num_samples=40, num_features=4)\n",
    "\n",
    "# Split training data (70%), validation data (15%), and test data (15%) \n",
    "X_train, y_train = X[:28], y[:28]\n",
    "X_val, y_val = X[28:34], y[28:34]\n",
    "X_test, y_test = X[34:], y[34:]\n",
    "\n",
    "# Build the FFNN model\n",
    "model = FFNN([\n",
    "    Dense(neuron_size=5, activation=\"sigmoid\", kernel_initializer=\"glorot_uniform\", input_size=4),\n",
    "    Dense(neuron_size=7, activation=\"sigmoid\", kernel_initializer=\"glorot_uniform\"),\n",
    "    Dense(neuron_size=1, activation=\"sigmoid\", kernel_initializer=\"glorot_uniform\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"sgd\", loss=\"mean_squared_error\")\n",
    "\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=20, validation_data=(X_val, y_val))\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'gradient'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[73]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m i = \u001b[32m1\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model.layers:\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mlayer-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m gradients:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mlayer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgradients\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgradient\u001b[49m)\n\u001b[32m      7\u001b[39m     i += \u001b[32m1\u001b[39m\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'gradient'"
     ]
    }
   ],
   "source": [
    "## Check gradients for each layer\n",
    "\n",
    "model.forward(Tensor(X_train[0]))\n",
    "model.backward(y_train[0])\n",
    "\n",
    "i = 1\n",
    "for layer in model.layers:\n",
    "    print(f\"layer-{i} gradients:\", layer.gradients)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **mnist_784 Test Case**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "X = X / 255.0\n",
    "\n",
    "y = y.astype(int)\n",
    "num_classes = np.max(y) + 1\n",
    "y = np.eye(num_classes)[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (49000, 784), (49000, 10)\n",
      "Validation set: (7000, 784), (7000, 10)\n",
      "Test set: (14000, 784), (14000, 10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.125, random_state=42, stratify=y_train)\n",
    "\n",
    "print(f\"Training set: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}, {y_val.shape}\")\n",
    "print(f\"Test set: {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation Loss: 0.22555433515142226\n",
      "Epoch 2, Validation Loss: 0.18832323709324095\n",
      "Epoch 3, Validation Loss: 0.5033724418890065\n",
      "Epoch 4, Validation Loss: 0.2753022073885225\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[70]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      1\u001b[39m model = FFNN([\n\u001b[32m      2\u001b[39m     Dense(neuron_size=\u001b[32m256\u001b[39m, activation=\u001b[33m\"\u001b[39m\u001b[33mrelu\u001b[39m\u001b[33m\"\u001b[39m, kernel_initializer=\u001b[33m\"\u001b[39m\u001b[33mglorot_uniform\u001b[39m\u001b[33m\"\u001b[39m, input_size=\u001b[32m784\u001b[39m),\n\u001b[32m      3\u001b[39m     Dense(neuron_size=\u001b[32m128\u001b[39m, activation=\u001b[33m\"\u001b[39m\u001b[33mrelu\u001b[39m\u001b[33m\"\u001b[39m, kernel_initializer=\u001b[33m\"\u001b[39m\u001b[33mglorot_uniform\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m      4\u001b[39m     Dense(neuron_size=\u001b[32m10\u001b[39m, activation=\u001b[33m\"\u001b[39m\u001b[33msoftmax\u001b[39m\u001b[33m\"\u001b[39m, kernel_initializer=\u001b[33m\"\u001b[39m\u001b[33mglorot_uniform\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m ])\n\u001b[32m      7\u001b[39m model.compile(optimizer=\u001b[33m\"\u001b[39m\u001b[33msgd\u001b[39m\u001b[33m\"\u001b[39m, loss=\u001b[33m\"\u001b[39m\u001b[33mcategorical_crossentropy\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m25\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m25\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# res = model.predict(X_train[:5])\u001b[39;00m\n\u001b[32m     11\u001b[39m \n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# print(res)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Irfan Sidiq\\Documents\\uni\\smt 6\\ml\\tugas\\IF3270_FFNN\\src\\model.py:134\u001b[39m, in \u001b[36mFFNN.fit\u001b[39m\u001b[34m(self, X_train, y_train, epochs, batch_size, validation_data)\u001b[39m\n\u001b[32m    132\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[32m    133\u001b[39m         \u001b[38;5;28mself\u001b[39m.forward(Tensor(sample[\u001b[32m0\u001b[39m])) \u001b[38;5;66;03m# X\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# y\u001b[39;00m\n\u001b[32m    136\u001b[39m     \u001b[38;5;28mself\u001b[39m.optimizer.step()\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m validation_data:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Irfan Sidiq\\Documents\\uni\\smt 6\\ml\\tugas\\IF3270_FFNN\\src\\model.py:108\u001b[39m, in \u001b[36mFFNN.backward\u001b[39m\u001b[34m(self, y_true)\u001b[39m\n\u001b[32m    105\u001b[39m     \u001b[38;5;28mself\u001b[39m.output._Tensor__backward = __backward\n\u001b[32m    107\u001b[39m loss = \u001b[38;5;28mself\u001b[39m.output.compute_loss(y_true, \u001b[38;5;28mself\u001b[39m.loss_function)\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Irfan Sidiq\\Documents\\uni\\smt 6\\ml\\tugas\\IF3270_FFNN\\src\\tensor.py:122\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(topo):\n\u001b[32m    121\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m v.requires_grad:\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m         v.__backward()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model = FFNN([\n",
    "    Dense(neuron_size=256, activation=\"relu\", kernel_initializer=\"glorot_uniform\", input_size=784),\n",
    "    Dense(neuron_size=128, activation=\"relu\", kernel_initializer=\"glorot_uniform\"),\n",
    "    Dense(neuron_size=10, activation=\"softmax\", kernel_initializer=\"glorot_uniform\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"sgd\", loss=\"categorical_crossentropy\")\n",
    "\n",
    "model.fit(X_train[:200], y_train[:200], 20, 20, (X_val[:25], y_val[:25]))\n",
    "# res = model.predict(X_train[:5])\n",
    "\n",
    "# print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, metric = model.evaluate(X_test[:300], y_test[:300])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
