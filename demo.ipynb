{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from src.tensor import Tensor\n",
    "from src.activation_function import Linear, ReLU, Sigmoid, Softmax\n",
    "from src.loss_function import MeanSquaredError\n",
    "from src.layer import Dense\n",
    "from src.model import FFNN\n",
    "from src.weight_initializer import GlorotUniformInitializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tensor Test Case**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value: [1. 2.], Gradient: [0. 0.], Op: None\n",
      "Value: [3. 4.], Gradient: [0. 0.], Op: None\n",
      "Value: [4. 6.], Gradient: [0. 0.], Op: +\n",
      "Value: [-2. -2.], Gradient: [0. 0.], Op: +\n",
      "Value: [3. 8.], Gradient: [0. 0.], Op: *\n",
      "Value: [0.33333333 0.5       ], Gradient: [0. 0.], Op: *\n"
     ]
    }
   ],
   "source": [
    "# Basic operations\n",
    "a = Tensor(np.array([1,2]))\n",
    "b = Tensor(np.array([3,4]))\n",
    "c = a + b\n",
    "d = a - b\n",
    "e = a * b\n",
    "f = a / b\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "print(d)\n",
    "print(e)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value: [1. 2.], Gradient: [0. 0.], Op: None\n",
      "Value: [1. 2.], Gradient: [0. 0.], Op: Linear\n",
      "Value: [1. 2.], Gradient: [0. 0.], Op: ReLU\n",
      "Value: [4.], Gradient: [0.], Op: MeanSquaredError\n",
      "Value: [4.], Gradient: [0.], Op: MeanSquaredError\n"
     ]
    }
   ],
   "source": [
    "# Activation function and loss function\n",
    "a = Tensor(np.array([1,2]))\n",
    "b = a.compute_activation(Linear)\n",
    "c = a.compute_activation(ReLU)\n",
    "d = b.compute_loss(Tensor(np.array([3,4])), MeanSquaredError)\n",
    "e = c.compute_loss(Tensor(np.array([3,4])), MeanSquaredError)\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "print(d)\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Before backpropagation ----------\n",
      "Value: [1. 2.], Gradient: [0. 0.], Op: None\n",
      "Value: [3. 4.], Gradient: [0. 0.], Op: None\n",
      "Value: [4. 6.], Gradient: [0. 0.], Op: +\n",
      "Value: [-2. -2.], Gradient: [0. 0.], Op: +\n",
      "Value: [ -8. -12.], Gradient: [0. 0.], Op: *\n",
      "Value: [ -8. -12.], Gradient: [0. 0.], Op: Linear\n",
      "Value: [0. 0.], Gradient: [0. 0.], Op: ReLU\n",
      "Value: [125.], Gradient: [0.], Op: MeanSquaredError\n",
      "Value: [1.], Gradient: [0.], Op: MeanSquaredError\n",
      "\n",
      "---------- After backpropagation ----------\n",
      "Value: [1. 2.], Gradient: [-108. -312.], Op: None\n",
      "Value: [3. 4.], Gradient: [108. 156.], Op: None\n",
      "Value: [4. 6.], Gradient: [ 72. 104.], Op: +\n",
      "Value: [-2. -2.], Gradient: [-144. -312.], Op: +\n",
      "Value: [ -8. -12.], Gradient: [-18. -26.], Op: *\n",
      "Value: [ -8. -12.], Gradient: [-18. -26.], Op: Linear\n",
      "Value: [0. 0.], Gradient: [-2. -2.], Op: ReLU\n",
      "Value: [125.], Gradient: [1.], Op: MeanSquaredError\n",
      "Value: [1.], Gradient: [1.], Op: MeanSquaredError\n"
     ]
    }
   ],
   "source": [
    "# Automatic differentiation\n",
    "a = Tensor(np.array([1,2]))\n",
    "b = Tensor(np.array([3,4]))\n",
    "c = a + b\n",
    "d = a - b\n",
    "e = c * d\n",
    "f = e.compute_activation(Linear)\n",
    "g = e.compute_activation(ReLU)\n",
    "h = f.compute_loss(np.array([1,1]), MeanSquaredError)\n",
    "i = g.compute_loss(np.array([1,1]), MeanSquaredError)\n",
    "\n",
    "print(\"---------- Before backpropagation ----------\")\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "print(d)\n",
    "print(e)\n",
    "print(f)\n",
    "print(g)\n",
    "print(h)\n",
    "print(i)\n",
    "\n",
    "h.backward()\n",
    "i.backward()\n",
    "\n",
    "print(\"\\n---------- After backpropagation ----------\")\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "print(d)\n",
    "print(e)\n",
    "print(f)\n",
    "print(g)\n",
    "print(h)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Before backpropagation ----------\n",
      "Value: [2. 3. 4.], Gradient: [0. 0. 0.], Op: None, Type: weight\n",
      "Value: [3. 4. 5.], Gradient: [0. 0. 0.], Op: None, Type: weight\n",
      "Value: [ 2.  6. 12.], Gradient: [0. 0. 0.], Op: *\n",
      "Value: [ 3.  8. 15.], Gradient: [0. 0. 0.], Op: *\n",
      "Value: [20.], Gradient: [0.], Op: sum\n",
      "Value: [26.], Gradient: [0.], Op: sum\n",
      "Value: [20.], Gradient: [0.], Op: ReLU\n",
      "Value: [26.], Gradient: [0.], Op: ReLU\n",
      "Value: [20. 26.], Gradient: [0. 0.], Op: concat\n",
      "Value: [80.], Gradient: [0.], Op: MeanSquaredError\n",
      "\n",
      "---------- After backpropagation ----------\n",
      "Value: [2. 3. 4.], Gradient: [ 8. 16. 24.], Op: None, Type: weight\n",
      "Value: [3. 4. 5.], Gradient: [24. 48. 72.], Op: None, Type: weight\n",
      "Value: [ 2.  6. 12.], Gradient: [8. 8. 8.], Op: *\n",
      "Value: [ 3.  8. 15.], Gradient: [24. 24. 24.], Op: *\n",
      "Value: [20.], Gradient: [8.], Op: sum\n",
      "Value: [26.], Gradient: [24.], Op: sum\n",
      "Value: [20.], Gradient: [8.], Op: ReLU\n",
      "Value: [26.], Gradient: [24.], Op: ReLU\n",
      "Value: [20. 26.], Gradient: [ 8. 24.], Op: concat\n",
      "Value: [80.], Gradient: [1.], Op: MeanSquaredError\n"
     ]
    }
   ],
   "source": [
    "## Simulation of one layer with two neurons (h1 and h2)\n",
    "\n",
    "# Initial values\n",
    "x = Tensor(np.array([1, 2, 3]), tensor_type=\"input\")            # input, x[0] is always 1\n",
    "y = np.array([16, 14])                                          # correct class / y_true\n",
    "wh1 = Tensor(np.array([2, 3, 4]), tensor_type=\"weight\")         # weights of neuron h1, wh1[0] = b1 (bias)\n",
    "wh2 = Tensor(np.array([3, 4, 5]), tensor_type=\"weight\")         # weights of neuron h2, wh2[0] = b2 (bias)\n",
    "\n",
    "# Calculate net\n",
    "wh1_x = wh1 * x\n",
    "wh2_x = wh2 * x\n",
    "net1 = wh1_x.sum()\n",
    "net2 = wh2_x.sum()\n",
    "\n",
    "# Calculate output\n",
    "o1 = net1.compute_activation(ReLU)\n",
    "o2 = net2.compute_activation(ReLU)\n",
    "\n",
    "# Calculate loss\n",
    "output = o1.concat([o2])\n",
    "loss = output.compute_loss(y, MeanSquaredError)\n",
    "\n",
    "print(\"---------- Before backpropagation ----------\")\n",
    "print(wh1)\n",
    "print(wh2)\n",
    "print(wh1_x)\n",
    "print(wh2_x)\n",
    "print(net1)\n",
    "print(net2)\n",
    "print(o1)\n",
    "print(o2)\n",
    "print(output)\n",
    "print(loss)\n",
    "\n",
    "# Initiate automated differentiation\n",
    "loss.backward()\n",
    "\n",
    "print(\"\\n---------- After backpropagation ----------\")\n",
    "print(wh1)\n",
    "print(wh2)\n",
    "print(wh1_x)\n",
    "print(wh2_x)\n",
    "print(net1)\n",
    "print(net2)\n",
    "print(o1)\n",
    "print(o2)\n",
    "print(output)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ Layer 1 ============\n",
      "Value: [ 2.  6. 12.], Gradient: [16291060. 16291060. 16291060.], Op: *\n",
      "Value: [ 3.  8. 15.], Gradient: [20034510. 20034510. 20034510.], Op: *\n",
      "Value: [ 4. 10. 18.], Gradient: [23777960. 23777960. 23777960.], Op: *\n",
      "Value: [20.], Gradient: [16291060.], Op: sum\n",
      "Value: [26.], Gradient: [20034510.], Op: sum\n",
      "Value: [32.], Gradient: [23777960.], Op: sum\n",
      "Value: [20.], Gradient: [16291060.], Op: ReLU\n",
      "Value: [26.], Gradient: [20034510.], Op: ReLU\n",
      "Value: [32.], Gradient: [23777960.], Op: ReLU\n",
      "Value: [ 1. 20. 26. 32.], Gradient: [16291060. 20034510. 23777960. 27521410.], Op: concat\n",
      "\n",
      "============ Layer 2 ============\n",
      "Value: [ 1. 20. 26. 32.], Gradient: [16291060. 20034510. 23777960. 27521410.], Op: concat\n",
      "Value: [  2.  60. 104. 160.], Gradient: [485238. 485238. 485238. 485238.], Op: *\n",
      "Value: [  3.  80. 130. 192.], Gradient: [616964. 616964. 616964. 616964.], Op: *\n",
      "Value: [  4. 100. 156. 224.], Gradient: [748690. 748690. 748690. 748690.], Op: *\n",
      "Value: [  5. 120. 182. 256.], Gradient: [880416. 880416. 880416. 880416.], Op: *\n",
      "Value: [  6. 140. 208. 288.], Gradient: [1012142. 1012142. 1012142. 1012142.], Op: *\n",
      "Value: [326.], Gradient: [485238.], Op: sum\n",
      "Value: [405.], Gradient: [616964.], Op: sum\n",
      "Value: [484.], Gradient: [748690.], Op: sum\n",
      "Value: [563.], Gradient: [880416.], Op: sum\n",
      "Value: [642.], Gradient: [1012142.], Op: sum\n",
      "Value: [326.], Gradient: [485238.], Op: ReLU\n",
      "Value: [405.], Gradient: [616964.], Op: ReLU\n",
      "Value: [484.], Gradient: [748690.], Op: ReLU\n",
      "Value: [563.], Gradient: [880416.], Op: ReLU\n",
      "Value: [642.], Gradient: [1012142.], Op: ReLU\n",
      "Value: [  1. 326. 405. 484. 563. 642.], Gradient: [ 485238.  616964.  748690.  880416. 1012142. 1143868.], Op: concat\n",
      "\n",
      "============ Layer 3 ============\n",
      "Value: [  1. 326. 405. 484. 563. 642.], Gradient: [ 485238.  616964.  748690.  880416. 1012142. 1143868.], Op: concat\n",
      "Value: [2.000e+00 9.780e+02 1.620e+03 2.420e+03 3.378e+03 4.494e+03], Gradient: [25684. 25684. 25684. 25684. 25684. 25684.], Op: *\n",
      "Value: [3.000e+00 1.304e+03 2.025e+03 2.904e+03 3.941e+03 5.136e+03], Gradient: [30498. 30498. 30498. 30498. 30498. 30498.], Op: *\n",
      "Value: [4.000e+00 1.630e+03 2.430e+03 3.388e+03 4.504e+03 5.778e+03], Gradient: [35344. 35344. 35344. 35344. 35344. 35344.], Op: *\n",
      "Value: [5.000e+00 1.956e+03 2.835e+03 3.872e+03 5.067e+03 6.420e+03], Gradient: [40200. 40200. 40200. 40200. 40200. 40200.], Op: *\n",
      "Value: [12892.], Gradient: [25684.], Op: sum\n",
      "Value: [15313.], Gradient: [30498.], Op: sum\n",
      "Value: [17734.], Gradient: [35344.], Op: sum\n",
      "Value: [20155.], Gradient: [40200.], Op: sum\n",
      "Value: [12892.], Gradient: [25684.], Op: ReLU\n",
      "Value: [15313.], Gradient: [30498.], Op: ReLU\n",
      "Value: [17734.], Gradient: [35344.], Op: ReLU\n",
      "Value: [20155.], Gradient: [40200.], Op: ReLU\n",
      "Value: [12892. 15313. 17734. 20155.], Gradient: [25684. 30498. 35344. 40200.], Op: concat\n",
      "Value: [2.78439637e+08], Gradient: [1.], Op: MeanSquaredError\n"
     ]
    }
   ],
   "source": [
    "## Simulation of 3-layered (excluding input layer) network with n = [3, 5, 4] number of neurons\n",
    "\n",
    "# Initial values\n",
    "x = Tensor(np.array([1, 2, 3]), tensor_type=\"input\")            \n",
    "y = np.array([50, 64, 62, 55])                                          \n",
    "wh1 = Tensor(np.array([2, 3, 4]), tensor_type=\"weight\")\n",
    "wh2 = Tensor(np.array([3, 4, 5]), tensor_type=\"weight\")\n",
    "wh3 = Tensor(np.array([4, 5, 6]), tensor_type=\"weight\")\n",
    "wh4 = Tensor(np.array([2, 3, 4, 5]), tensor_type=\"weight\")\n",
    "wh5 = Tensor(np.array([3, 4, 5, 6]), tensor_type=\"weight\")\n",
    "wh6 = Tensor(np.array([4, 5, 6, 7]), tensor_type=\"weight\")\n",
    "wh7 = Tensor(np.array([5, 6, 7, 8]), tensor_type=\"weight\")\n",
    "wh8 = Tensor(np.array([6, 7, 8, 9]), tensor_type=\"weight\")\n",
    "wh9 = Tensor(np.array([2, 3, 4, 5, 6, 7]), tensor_type=\"weight\")\n",
    "wh10 = Tensor(np.array([3, 4, 5, 6, 7, 8]), tensor_type=\"weight\")\n",
    "wh11 = Tensor(np.array([4, 5, 6, 7, 8, 9]), tensor_type=\"weight\")\n",
    "wh12 = Tensor(np.array([5, 6, 7, 8, 9, 10]), tensor_type=\"weight\")\n",
    "\n",
    "\n",
    "# Layer 1\n",
    "wh1_x = wh1 * x\n",
    "wh2_x = wh2 * x\n",
    "wh3_x = wh3 * x\n",
    "net1 = wh1_x.sum()\n",
    "net2 = wh2_x.sum()\n",
    "net3 = wh3_x.sum()\n",
    "o1 = net1.compute_activation(ReLU)\n",
    "o2 = net2.compute_activation(ReLU)\n",
    "o3 = net3.compute_activation(ReLU)\n",
    "output_l1 = o1.concat([o2, o3])\n",
    "\n",
    "# Layer 2\n",
    "input_l2 = output_l1.add_x0()\n",
    "wh4_l2 = wh4 * input_l2\n",
    "wh5_l2 = wh5 * input_l2\n",
    "wh6_l2 = wh6 * input_l2\n",
    "wh7_l2 = wh7 * input_l2\n",
    "wh8_l2 = wh8 * input_l2\n",
    "net4 = wh4_l2.sum()\n",
    "net5 = wh5_l2.sum()\n",
    "net6 = wh6_l2.sum()\n",
    "net7 = wh7_l2.sum()\n",
    "net8 = wh8_l2.sum()\n",
    "o4 = net4.compute_activation(ReLU)\n",
    "o5 = net5.compute_activation(ReLU)\n",
    "o6 = net6.compute_activation(ReLU)\n",
    "o7 = net7.compute_activation(ReLU)\n",
    "o8 = net8.compute_activation(ReLU)\n",
    "output_l2 = o4.concat([o5, o6, o7, o8])\n",
    "\n",
    "# Layer 3\n",
    "input_l3 = output_l2.add_x0()\n",
    "wh9_l3 = wh9 * input_l3\n",
    "wh10_l3 = wh10 * input_l3\n",
    "wh11_l3 = wh11 * input_l3\n",
    "wh12_l3 = wh12 * input_l3\n",
    "net9 = wh9_l3.sum()\n",
    "net10 = wh10_l3.sum()\n",
    "net11 = wh11_l3.sum()\n",
    "net12 = wh12_l3.sum()\n",
    "o9 = net9.compute_activation(ReLU)\n",
    "o10 = net10.compute_activation(ReLU)\n",
    "o11 = net11.compute_activation(ReLU)\n",
    "o12 = net12.compute_activation(ReLU)\n",
    "output_l3 = o9.concat([o10, o11, o12])\n",
    "loss = output_l3.compute_loss(y, MeanSquaredError)\n",
    "\n",
    "\n",
    "# Backpropagation\n",
    "loss.backward()\n",
    "\n",
    "print(\"============ Layer 1 ============\")\n",
    "print(wh1_x)\n",
    "print(wh2_x)\n",
    "print(wh3_x)\n",
    "print(net1)\n",
    "print(net2)\n",
    "print(net3)\n",
    "print(o1)\n",
    "print(o2)\n",
    "print(o3)\n",
    "print(output_l1)\n",
    "print(\"\\n============ Layer 2 ============\")\n",
    "print(input_l2)\n",
    "print(wh4_l2)\n",
    "print(wh5_l2)\n",
    "print(wh6_l2)\n",
    "print(wh7_l2)\n",
    "print(wh8_l2)\n",
    "print(net4)\n",
    "print(net5)\n",
    "print(net6)\n",
    "print(net7)\n",
    "print(net8)\n",
    "print(o4)\n",
    "print(o5)\n",
    "print(o6)\n",
    "print(o7)\n",
    "print(o8)\n",
    "print(output_l2)\n",
    "print(\"\\n============ Layer 3 ============\")\n",
    "print(input_l3)\n",
    "print(wh9_l3)\n",
    "print(wh10_l3)\n",
    "print(wh11_l3)\n",
    "print(wh12_l3)\n",
    "print(net9)\n",
    "print(net10)\n",
    "print(net11)\n",
    "print(net12)\n",
    "print(o9)\n",
    "print(o10)\n",
    "print(o11)\n",
    "print(o12)\n",
    "print(output_l3)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Layer Test Case**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred: Value: [-2.28833063 -0.91082992  3.24960389 -0.03329379  0.7034286 ], Gradient: [0. 0. 0. 0. 0.], Op: concat\n",
      "Loss: Value: [11.35209401], Gradient: [0.], Op: MeanSquaredError\n",
      "\n",
      "---------- Before backpropagation ----------\n",
      "Weights: [Value: [ 0.21898912  0.48338857 -0.18376022 -0.73070405], Gradient: [0. 0. 0. 0.], Op: None, Type: weight, Value: [-0.33975298 -0.17490727  0.43756951 -0.38349273], Gradient: [0. 0. 0. 0.], Op: None, Type: weight, Value: [ 0.380372   -0.14919553  0.30283812  0.56477715], Gradient: [0. 0. 0. 0.], Op: None, Type: weight, Value: [ 0.78799295 -0.44396825  0.22573212 -0.15263665], Gradient: [0. 0. 0. 0.], Op: None, Type: weight, Value: [ 0.64913492 -0.0870846  -0.42925467  0.37905672], Gradient: [0. 0. 0. 0.], Op: None, Type: weight]\n",
      "\n",
      "---------- After backpropagation ----------\n",
      "Weights: [Value: [ 0.21898912  0.48338857 -0.18376022 -0.73070405], Gradient: [-14.57666126 -29.15332252 -43.72998377 -58.30664503], Op: None, Type: weight, Value: [-0.33975298 -0.17490727  0.43756951 -0.38349273], Gradient: [-1.82165984 -3.64331969 -5.46497953 -7.28663937], Op: None, Type: weight, Value: [ 0.380372   -0.14919553  0.30283812  0.56477715], Gradient: [0.49920778 0.99841556 1.49762334 1.99683112], Op: None, Type: weight, Value: [ 0.78799295 -0.44396825  0.22573212 -0.15263665], Gradient: [-2.06658759 -4.13317517 -6.19976276 -8.26635034], Op: None, Type: weight, Value: [ 0.64913492 -0.0870846  -0.42925467  0.37905672], Gradient: [ -2.5931428   -5.1862856   -7.77942841 -10.37257121], Op: None, Type: weight]\n"
     ]
    }
   ],
   "source": [
    "## Simulation of one-layered network with n = 5 number of neurons\n",
    "\n",
    "x = Tensor(np.array([1, 2, 3, 4]))      # input, x[0] is always 1\n",
    "y = np.array([5, 0, 3, 1, 2])           # correct class / y_true\n",
    "\n",
    "layer = Dense(neuron_size=5, activation=\"linear\", kernel_initializer=\"glorot_uniform\", input_size=3) # output layer with 5 neurons\n",
    "\n",
    "# Forwardpropagation\n",
    "y_pred = layer.forward(x)\n",
    "loss = y_pred.compute_loss(y, MeanSquaredError)\n",
    "print(\"y_pred:\", y_pred)\n",
    "print(\"Loss:\", loss)\n",
    "\n",
    "print(\"\\n---------- Before backpropagation ----------\")\n",
    "print('Weights:', layer.weights)\n",
    "\n",
    "# Backpropagation\n",
    "loss.backward()\n",
    "print(\"\\n---------- After backpropagation ----------\")\n",
    "print(\"Weights:\", layer.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **FFNN Test Case**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred: [ 1.08280691 -3.9456623  -2.89536147  0.1779078   0.1844783 ]\n",
      "\n",
      "---------- Before backpropagation ----------\n",
      "[Value: [-0.14196564 -0.22094583  0.68587401 -0.09773946], Gradient: [0. 0. 0. 0.], Op: None, Type: weight, Value: [-0.55022965 -0.50279027 -0.23835301 -0.41869827], Gradient: [0. 0. 0. 0.], Op: None, Type: weight, Value: [ 0.34080087 -0.47776755  0.30501036 -0.79891458], Gradient: [0. 0. 0. 0.], Op: None, Type: weight, Value: [-0.33538684  0.01071245 -0.60909573  0.57978923], Gradient: [0. 0. 0. 0.], Op: None, Type: weight, Value: [-0.09143406 -0.04448454  0.73113813 -0.45713324], Gradient: [0. 0. 0. 0.], Op: None, Type: weight]\n",
      "\n",
      "---------- After backpropagation ----------\n",
      "[Value: [-0.14196564 -0.22094583  0.68587401 -0.09773946], Gradient: [ -7.83438618 -15.66877236 -23.50315855 -31.33754473], Op: None, Type: weight, Value: [-0.55022965 -0.50279027 -0.23835301 -0.41869827], Gradient: [ -7.8913246  -15.78264921 -23.67397381 -31.56529841], Op: None, Type: weight, Value: [ 0.34080087 -0.47776755  0.30501036 -0.79891458], Gradient: [-11.79072295 -23.5814459  -35.37216884 -47.16289179], Op: None, Type: weight, Value: [-0.33538684  0.01071245 -0.60909573  0.57978923], Gradient: [-1.6441844  -3.28836881 -4.93255321 -6.57673761], Op: None, Type: weight, Value: [-0.09143406 -0.04448454  0.73113813 -0.45713324], Gradient: [ -3.63104341  -7.26208682 -10.89313023 -14.52417363], Op: None, Type: weight]\n"
     ]
    }
   ],
   "source": [
    "## Simulation of one-layered network with n = 5 number of neurons\n",
    "\n",
    "x = Tensor(np.array([2, 3, 4]))      \n",
    "y = np.array([5, 0, 3, 1, 2])           \n",
    "\n",
    "# Build the FFNN model\n",
    "model = FFNN([\n",
    "    Dense(neuron_size=5, activation=\"linear\", kernel_initializer=\"glorot_uniform\", input_size=3)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"sgd\", loss=\"mean_squared_error\")\n",
    "\n",
    "# Predict output (without fitting)\n",
    "y_pred = model.forward(x)\n",
    "print(\"y_pred:\", y_pred)\n",
    "print(\"\\n---------- Before backpropagation ----------\")\n",
    "for layer in model.layers:\n",
    "    print(layer.weights)\n",
    "\n",
    "# Initiate backwardpropagation\n",
    "model.backward(y)\n",
    "print(\"\\n---------- After backpropagation ----------\")\n",
    "for layer in model.layers:\n",
    "    print(layer.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_linearly_separable_data(num_samples, num_features=2, weight=None, bias=1, random_seed=None):\n",
    "    if num_features < 2:\n",
    "        raise ValueError(\"num_features must be at least 2 for meaningful separation.\")\n",
    "    \n",
    "    if random_seed is not None:\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "    X = np.random.rand(num_samples, num_features) * 20\n",
    "\n",
    "    if weight is None:\n",
    "        weight = np.random.uniform(-5, 5, size=(num_features - 1))\n",
    "\n",
    "    decision_boundary = np.dot(X[:, :-1], weight) + X[:, -1]\n",
    "    y = np.array(decision_boundary > (bias + 10), dtype=int).reshape(-1, 1)\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation Loss: 0.05747100228003801\n",
      "Epoch 2, Validation Loss: 0.05204575526911812\n",
      "Epoch 3, Validation Loss: 0.04745240307360599\n",
      "Epoch 4, Validation Loss: 0.04352482676954859\n",
      "Epoch 5, Validation Loss: 0.040136375523001276\n",
      "Epoch 6, Validation Loss: 0.03718927436076861\n",
      "Epoch 7, Validation Loss: 0.03460713248391065\n",
      "Epoch 8, Validation Loss: 0.032329587458457536\n",
      "Epoch 9, Validation Loss: 0.030308432683792203\n",
      "Epoch 10, Validation Loss: 0.028504785140673103\n",
      "Epoch 11, Validation Loss: 0.02688699052562249\n",
      "Epoch 12, Validation Loss: 0.02542905675091744\n",
      "Epoch 13, Validation Loss: 0.02410947009651354\n",
      "Epoch 14, Validation Loss: 0.02291029137530343\n",
      "Epoch 15, Validation Loss: 0.021816459077815476\n",
      "Epoch 16, Validation Loss: 0.020815247019429214\n",
      "Epoch 17, Validation Loss: 0.019895838429354384\n",
      "Epoch 18, Validation Loss: 0.019048988623792045\n",
      "Epoch 19, Validation Loss: 0.0182667556890689\n",
      "Epoch 20, Validation Loss: 0.017542283842223894\n",
      "Epoch 21, Validation Loss: 0.016869627939453645\n",
      "Epoch 22, Validation Loss: 0.016243610384750678\n",
      "Epoch 23, Validation Loss: 0.015659703743509102\n",
      "Epoch 24, Validation Loss: 0.015113933893397352\n",
      "Epoch 25, Validation Loss: 0.014602799691717907\n",
      "Epoch 26, Validation Loss: 0.014123206007155284\n",
      "Epoch 27, Validation Loss: 0.013672407627329089\n",
      "Epoch 28, Validation Loss: 0.013247962064463645\n",
      "Epoch 29, Validation Loss: 0.012847689677851097\n",
      "Epoch 30, Validation Loss: 0.012469639841472613\n",
      "Epoch 31, Validation Loss: 0.01211206212870168\n",
      "Epoch 32, Validation Loss: 0.011773381678738154\n",
      "Epoch 33, Validation Loss: 0.011452178062791099\n",
      "Epoch 34, Validation Loss: 0.01114716709073108\n",
      "Epoch 35, Validation Loss: 0.010857185097592\n",
      "Epoch 36, Validation Loss: 0.010581175329005208\n",
      "Epoch 37, Validation Loss: 0.010318176109325432\n",
      "Epoch 38, Validation Loss: 0.01006731052891617\n",
      "Epoch 39, Validation Loss: 0.009827777430190877\n",
      "Epoch 40, Validation Loss: 0.009598843507434757\n",
      "Epoch 41, Validation Loss: 0.009379836364643922\n",
      "Epoch 42, Validation Loss: 0.009170138399791748\n",
      "Epoch 43, Validation Loss: 0.008969181404005357\n",
      "Epoch 44, Validation Loss: 0.0087764417808592\n",
      "Epoch 45, Validation Loss: 0.008591436304972479\n",
      "Epoch 46, Validation Loss: 0.008413718350818179\n",
      "Epoch 47, Validation Loss: 0.008242874532510616\n",
      "Epoch 48, Validation Loss: 0.008078521703653807\n",
      "Epoch 49, Validation Loss: 0.007920304273367802\n",
      "Epoch 50, Validation Loss: 0.007767891800577673\n",
      "[array([0.08909421]), array([0.09012391]), array([0.07988406]), array([0.07961739]), array([0.08819846]), array([0.09705754])]\n"
     ]
    }
   ],
   "source": [
    "## Simulation of three-layered network with n = [5, 7, 3] number of neurons, with fiting phase\n",
    "\n",
    "# Generate dataset\n",
    "X, y = generate_linearly_separable_data(num_samples=40, num_features=4)\n",
    "\n",
    "# Split training data (70%), validation data (15%), and test data (15%) \n",
    "X_train, y_train = X[:28], y[:28]\n",
    "X_val, y_val = X[28:34], y[28:34]\n",
    "X_test, y_test = X[34:], y[34:]\n",
    "\n",
    "# Build the FFNN model\n",
    "model = FFNN([\n",
    "    Dense(neuron_size=5, activation=\"sigmoid\", kernel_initializer=\"glorot_uniform\", input_size=4),\n",
    "    Dense(neuron_size=7, activation=\"sigmoid\", kernel_initializer=\"glorot_uniform\"),\n",
    "    Dense(neuron_size=1, activation=\"sigmoid\", kernel_initializer=\"glorot_uniform\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"sgd\", loss=\"mean_squared_error\")\n",
    "\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=20, validation_data=(X_val, y_val))\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **mnist_784 Test Case**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "X = X / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to integers\n",
    "y = y.astype(int)\n",
    "\n",
    "# One-hot encode using np.eye()\n",
    "num_classes = np.max(y) + 1\n",
    "y = np.eye(num_classes)[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (49000, 784), (49000, 10)\n",
      "Validation set: (7000, 784), (7000, 10)\n",
      "Test set: (14000, 784), (14000, 10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.125, random_state=42, stratify=y_train)\n",
    "\n",
    "print(f\"Training set: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}, {y_val.shape}\")\n",
    "print(f\"Test set: {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation Loss: 0.2010598098106946\n",
      "Epoch 2, Validation Loss: 0.1827466372219712\n",
      "Epoch 3, Validation Loss: 0.1486053066817694\n",
      "Epoch 4, Validation Loss: 0.07663933945462023\n",
      "Epoch 5, Validation Loss: 0.061013123063173164\n",
      "Epoch 6, Validation Loss: 0.046962566726295875\n",
      "Epoch 7, Validation Loss: 0.04861137161869134\n",
      "Epoch 8, Validation Loss: 0.035842108554138497\n",
      "Epoch 9, Validation Loss: 0.03838128986536981\n",
      "Epoch 10, Validation Loss: 0.025256349296199382\n",
      "Epoch 11, Validation Loss: 0.024474114569042554\n",
      "Epoch 12, Validation Loss: 0.025830198551818055\n",
      "Epoch 13, Validation Loss: 0.025622058823238425\n",
      "Epoch 14, Validation Loss: 0.02739135317970347\n",
      "Epoch 15, Validation Loss: 0.027064144541394067\n",
      "Epoch 16, Validation Loss: 0.024548986360234674\n",
      "Epoch 17, Validation Loss: 0.02319822655472346\n",
      "Epoch 18, Validation Loss: 0.022672644187608736\n",
      "Epoch 19, Validation Loss: 0.022307857267606434\n",
      "Epoch 20, Validation Loss: 0.022037734855405867\n"
     ]
    }
   ],
   "source": [
    "model = FFNN([\n",
    "    Dense(neuron_size=256, activation=\"relu\", kernel_initializer=\"glorot_uniform\", input_size=784),\n",
    "    Dense(neuron_size=128, activation=\"relu\", kernel_initializer=\"glorot_uniform\"),\n",
    "    Dense(neuron_size=10, activation=\"softmax\", kernel_initializer=\"glorot_uniform\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"sgd\", loss=\"categorical_crossentropy\")\n",
    "\n",
    "model.fit(X_train[:120], y_train[:120], 20, 20, (X_val[:20], y_val[:20]))\n",
    "# res = model.predict(X_train[:5])\n",
    "\n",
    "# print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0716, Accuracy: 0.8000\n"
     ]
    }
   ],
   "source": [
    "loss, metric = model.evaluate(X_test[:20], y_test[:20])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
