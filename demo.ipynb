{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from src.tensor import Tensor\n",
    "from src.activation_function import Linear, ReLU, Sigmoid\n",
    "from src.loss_function import MeanSquaredError\n",
    "from src.layer import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tensor Test Case**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value: [1. 2.], Gradient: [0. 0.], Op: None\n",
      "Value: [3. 4.], Gradient: [0. 0.], Op: None\n",
      "Value: [4. 6.], Gradient: [0. 0.], Op: +\n",
      "Value: [-2. -2.], Gradient: [0. 0.], Op: +\n",
      "Value: [3. 8.], Gradient: [0. 0.], Op: *\n",
      "Value: [0.33333333 0.5       ], Gradient: [0. 0.], Op: *\n"
     ]
    }
   ],
   "source": [
    "# Basic operations\n",
    "a = Tensor(np.array([1,2]))\n",
    "b = Tensor(np.array([3,4]))\n",
    "c = a + b\n",
    "d = a - b\n",
    "e = a * b\n",
    "f = a / b\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "print(d)\n",
    "print(e)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value: [1. 2.], Gradient: [0. 0.], Op: None\n",
      "Value: [1. 2.], Gradient: [0. 0.], Op: Linear\n",
      "Value: [1. 2.], Gradient: [0. 0.], Op: ReLU\n",
      "Value: [4.], Gradient: [0.], Op: MeanSquaredError\n",
      "Value: [4.], Gradient: [0.], Op: MeanSquaredError\n"
     ]
    }
   ],
   "source": [
    "# Activation function and loss function\n",
    "a = Tensor(np.array([1,2]))\n",
    "b = a.compute_activation(Linear)\n",
    "c = a.compute_activation(ReLU)\n",
    "d = b.compute_loss(Tensor(np.array([3,4])), MeanSquaredError)\n",
    "e = c.compute_loss(Tensor(np.array([3,4])), MeanSquaredError)\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "print(d)\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Before backpropagation ----------\n",
      "Value: [1. 2.], Gradient: [0. 0.], Op: None\n",
      "Value: [3. 4.], Gradient: [0. 0.], Op: None\n",
      "Value: [4. 6.], Gradient: [0. 0.], Op: +\n",
      "Value: [-2. -2.], Gradient: [0. 0.], Op: +\n",
      "Value: [ -8. -12.], Gradient: [0. 0.], Op: *\n",
      "Value: [ -8. -12.], Gradient: [0. 0.], Op: Linear\n",
      "Value: [0. 0.], Gradient: [0. 0.], Op: ReLU\n",
      "Value: [125.], Gradient: [0.], Op: MeanSquaredError\n",
      "Value: [1.], Gradient: [0.], Op: MeanSquaredError\n",
      "\n",
      "---------- After backpropagation ----------\n",
      "Value: [1. 2.], Gradient: [ -54. -156.], Op: None\n",
      "Value: [3. 4.], Gradient: [54. 78.], Op: None\n",
      "Value: [4. 6.], Gradient: [36. 52.], Op: +\n",
      "Value: [-2. -2.], Gradient: [ -72. -156.], Op: +\n",
      "Value: [ -8. -12.], Gradient: [ -9. -13.], Op: *\n",
      "Value: [ -8. -12.], Gradient: [ -9. -13.], Op: Linear\n",
      "Value: [0. 0.], Gradient: [-1. -1.], Op: ReLU\n",
      "Value: [125.], Gradient: [1.], Op: MeanSquaredError\n",
      "Value: [1.], Gradient: [1.], Op: MeanSquaredError\n"
     ]
    }
   ],
   "source": [
    "# Automatic differentiation\n",
    "a = Tensor(np.array([1,2]))\n",
    "b = Tensor(np.array([3,4]))\n",
    "c = a + b\n",
    "d = a - b\n",
    "e = c * d\n",
    "f = e.compute_activation(Linear)\n",
    "g = e.compute_activation(ReLU)\n",
    "h = f.compute_loss(np.array([1,1]), MeanSquaredError)\n",
    "i = g.compute_loss(np.array([1,1]), MeanSquaredError)\n",
    "\n",
    "print(\"---------- Before backpropagation ----------\")\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "print(d)\n",
    "print(e)\n",
    "print(f)\n",
    "print(g)\n",
    "print(h)\n",
    "print(i)\n",
    "\n",
    "h.backward()\n",
    "i.backward()\n",
    "\n",
    "print(\"\\n---------- After backpropagation ----------\")\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "print(d)\n",
    "print(e)\n",
    "print(f)\n",
    "print(g)\n",
    "print(h)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Before backpropagation ----------\n",
      "Value: [2. 3. 4.], Gradient: [0. 0. 0.], Op: None, Type: weight\n",
      "Value: [3. 4. 5.], Gradient: [0. 0. 0.], Op: None, Type: weight\n",
      "Value: [ 2.  6. 12.], Gradient: [0. 0. 0.], Op: *\n",
      "Value: [ 3.  8. 15.], Gradient: [0. 0. 0.], Op: *\n",
      "Value: [20.], Gradient: [0.], Op: sum\n",
      "Value: [26.], Gradient: [0.], Op: sum\n",
      "Value: [20.], Gradient: [0.], Op: ReLU\n",
      "Value: [26.], Gradient: [0.], Op: ReLU\n",
      "Value: [20. 26.], Gradient: [0. 0.], Op: concat\n",
      "Value: [80.], Gradient: [0.], Op: MeanSquaredError\n",
      "\n",
      "---------- After backpropagation ----------\n",
      "Value: [2. 3. 4.], Gradient: [ 4.  8. 12.], Op: None, Type: weight\n",
      "Value: [3. 4. 5.], Gradient: [12. 24. 36.], Op: None, Type: weight\n",
      "Value: [ 2.  6. 12.], Gradient: [4. 4. 4.], Op: *\n",
      "Value: [ 3.  8. 15.], Gradient: [12. 12. 12.], Op: *\n",
      "Value: [20.], Gradient: [4.], Op: sum\n",
      "Value: [26.], Gradient: [12.], Op: sum\n",
      "Value: [20.], Gradient: [4.], Op: ReLU\n",
      "Value: [26.], Gradient: [12.], Op: ReLU\n",
      "Value: [20. 26.], Gradient: [ 4. 12.], Op: concat\n",
      "Value: [80.], Gradient: [1.], Op: MeanSquaredError\n"
     ]
    }
   ],
   "source": [
    "## Simulation of one layer with two neurons (h1 and h2)\n",
    "\n",
    "# Initial values\n",
    "x = Tensor(np.array([1, 2, 3]), tensor_type=\"input\")            # input, x[0] is always 1\n",
    "y = np.array([16, 14])                                          # correct class / y_true\n",
    "wh1 = Tensor(np.array([2, 3, 4]), tensor_type=\"weight\")         # weights of neuron h1, wh1[0] = b1 (bias)\n",
    "wh2 = Tensor(np.array([3, 4, 5]), tensor_type=\"weight\")         # weights of neuron h2, wh2[0] = b2 (bias)\n",
    "\n",
    "# Calculate net\n",
    "wh1_x = wh1 * x\n",
    "wh2_x = wh2 * x\n",
    "net1 = wh1_x.sum()\n",
    "net2 = wh2_x.sum()\n",
    "\n",
    "# Calculate output\n",
    "o1 = net1.compute_activation(ReLU)\n",
    "o2 = net2.compute_activation(ReLU)\n",
    "\n",
    "# Calculate loss\n",
    "output = o1.concat([o2])\n",
    "loss = output.compute_loss(y, MeanSquaredError)\n",
    "\n",
    "print(\"---------- Before backpropagation ----------\")\n",
    "print(wh1)\n",
    "print(wh2)\n",
    "print(wh1_x)\n",
    "print(wh2_x)\n",
    "print(net1)\n",
    "print(net2)\n",
    "print(o1)\n",
    "print(o2)\n",
    "print(output)\n",
    "print(loss)\n",
    "\n",
    "# Initiate automated differentiation\n",
    "loss.backward()\n",
    "\n",
    "print(\"\\n---------- After backpropagation ----------\")\n",
    "print(wh1)\n",
    "print(wh2)\n",
    "print(wh1_x)\n",
    "print(wh2_x)\n",
    "print(net1)\n",
    "print(net2)\n",
    "print(o1)\n",
    "print(o2)\n",
    "print(output)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ Layer 1 ============\n",
      "Value: [ 2.  6. 12.], Gradient: [4072765. 4072765. 4072765.], Op: *\n",
      "Value: [ 3.  8. 15.], Gradient: [5008627.5 5008627.5 5008627.5], Op: *\n",
      "Value: [ 4. 10. 18.], Gradient: [5944490. 5944490. 5944490.], Op: *\n",
      "Value: [20.], Gradient: [4072765.], Op: sum\n",
      "Value: [26.], Gradient: [5008627.5], Op: sum\n",
      "Value: [32.], Gradient: [5944490.], Op: sum\n",
      "Value: [20.], Gradient: [4072765.], Op: ReLU\n",
      "Value: [26.], Gradient: [5008627.5], Op: ReLU\n",
      "Value: [32.], Gradient: [5944490.], Op: ReLU\n",
      "Value: [ 1. 20. 26. 32.], Gradient: [4072765.  5008627.5 5944490.  6880352.5], Op: concat\n",
      "============ Layer 2 ============\n",
      "Value: [ 1. 20. 26. 32.], Gradient: [4072765.  5008627.5 5944490.  6880352.5], Op: concat\n",
      "Value: [  2.  60. 104. 160.], Gradient: [121309.5 121309.5 121309.5 121309.5], Op: *\n",
      "Value: [  3.  80. 130. 192.], Gradient: [154241. 154241. 154241. 154241.], Op: *\n",
      "Value: [  4. 100. 156. 224.], Gradient: [187172.5 187172.5 187172.5 187172.5], Op: *\n",
      "Value: [  5. 120. 182. 256.], Gradient: [220104. 220104. 220104. 220104.], Op: *\n",
      "Value: [  6. 140. 208. 288.], Gradient: [253035.5 253035.5 253035.5 253035.5], Op: *\n",
      "Value: [326.], Gradient: [121309.5], Op: sum\n",
      "Value: [405.], Gradient: [154241.], Op: sum\n",
      "Value: [484.], Gradient: [187172.5], Op: sum\n",
      "Value: [563.], Gradient: [220104.], Op: sum\n",
      "Value: [642.], Gradient: [253035.5], Op: sum\n",
      "Value: [326.], Gradient: [121309.5], Op: ReLU\n",
      "Value: [405.], Gradient: [154241.], Op: ReLU\n",
      "Value: [484.], Gradient: [187172.5], Op: ReLU\n",
      "Value: [563.], Gradient: [220104.], Op: ReLU\n",
      "Value: [642.], Gradient: [253035.5], Op: ReLU\n",
      "Value: [  1. 326. 405. 484. 563. 642.], Gradient: [121309.5 154241.  187172.5 220104.  253035.5 285967. ], Op: concat\n",
      "============ Layer 3 ============\n",
      "Value: [  1. 326. 405. 484. 563. 642.], Gradient: [121309.5 154241.  187172.5 220104.  253035.5 285967. ], Op: concat\n",
      "Value: [2.000e+00 9.780e+02 1.620e+03 2.420e+03 3.378e+03 4.494e+03], Gradient: [6421. 6421. 6421. 6421. 6421. 6421.], Op: *\n",
      "Value: [3.000e+00 1.304e+03 2.025e+03 2.904e+03 3.941e+03 5.136e+03], Gradient: [7624.5 7624.5 7624.5 7624.5 7624.5 7624.5], Op: *\n",
      "Value: [4.000e+00 1.630e+03 2.430e+03 3.388e+03 4.504e+03 5.778e+03], Gradient: [8836. 8836. 8836. 8836. 8836. 8836.], Op: *\n",
      "Value: [5.000e+00 1.956e+03 2.835e+03 3.872e+03 5.067e+03 6.420e+03], Gradient: [10050. 10050. 10050. 10050. 10050. 10050.], Op: *\n",
      "Value: [12892.], Gradient: [6421.], Op: sum\n",
      "Value: [15313.], Gradient: [7624.5], Op: sum\n",
      "Value: [17734.], Gradient: [8836.], Op: sum\n",
      "Value: [20155.], Gradient: [10050.], Op: sum\n",
      "Value: [12892.], Gradient: [6421.], Op: ReLU\n",
      "Value: [15313.], Gradient: [7624.5], Op: ReLU\n",
      "Value: [17734.], Gradient: [8836.], Op: ReLU\n",
      "Value: [20155.], Gradient: [10050.], Op: ReLU\n",
      "Value: [12892. 15313. 17734. 20155.], Gradient: [ 6421.   7624.5  8836.  10050. ], Op: concat\n",
      "Value: [2.78439637e+08], Gradient: [1.], Op: MeanSquaredError\n"
     ]
    }
   ],
   "source": [
    "## Simulation of 3-layered (excluding input layer) network with n = [3, 5, 4] number of neurons\n",
    "\n",
    "# Initial values\n",
    "x = Tensor(np.array([1, 2, 3]), tensor_type=\"input\")            \n",
    "y = np.array([50, 64, 62, 55])                                          \n",
    "wh1 = Tensor(np.array([2, 3, 4]), tensor_type=\"weight\")\n",
    "wh2 = Tensor(np.array([3, 4, 5]), tensor_type=\"weight\")\n",
    "wh3 = Tensor(np.array([4, 5, 6]), tensor_type=\"weight\")\n",
    "wh4 = Tensor(np.array([2, 3, 4, 5]), tensor_type=\"weight\")\n",
    "wh5 = Tensor(np.array([3, 4, 5, 6]), tensor_type=\"weight\")\n",
    "wh6 = Tensor(np.array([4, 5, 6, 7]), tensor_type=\"weight\")\n",
    "wh7 = Tensor(np.array([5, 6, 7, 8]), tensor_type=\"weight\")\n",
    "wh8 = Tensor(np.array([6, 7, 8, 9]), tensor_type=\"weight\")\n",
    "wh9 = Tensor(np.array([2, 3, 4, 5, 6, 7]), tensor_type=\"weight\")\n",
    "wh10 = Tensor(np.array([3, 4, 5, 6, 7, 8]), tensor_type=\"weight\")\n",
    "wh11 = Tensor(np.array([4, 5, 6, 7, 8, 9]), tensor_type=\"weight\")\n",
    "wh12 = Tensor(np.array([5, 6, 7, 8, 9, 10]), tensor_type=\"weight\")\n",
    "\n",
    "\n",
    "# Layer 1\n",
    "wh1_x = wh1 * x\n",
    "wh2_x = wh2 * x\n",
    "wh3_x = wh3 * x\n",
    "net1 = wh1_x.sum()\n",
    "net2 = wh2_x.sum()\n",
    "net3 = wh3_x.sum()\n",
    "o1 = net1.compute_activation(ReLU)\n",
    "o2 = net2.compute_activation(ReLU)\n",
    "o3 = net3.compute_activation(ReLU)\n",
    "output_l1 = o1.concat([o2, o3])\n",
    "\n",
    "# Layer 2\n",
    "input_l2 = output_l1.add_x0()\n",
    "wh4_l2 = wh4 * input_l2\n",
    "wh5_l2 = wh5 * input_l2\n",
    "wh6_l2 = wh6 * input_l2\n",
    "wh7_l2 = wh7 * input_l2\n",
    "wh8_l2 = wh8 * input_l2\n",
    "net4 = wh4_l2.sum()\n",
    "net5 = wh5_l2.sum()\n",
    "net6 = wh6_l2.sum()\n",
    "net7 = wh7_l2.sum()\n",
    "net8 = wh8_l2.sum()\n",
    "o4 = net4.compute_activation(ReLU)\n",
    "o5 = net5.compute_activation(ReLU)\n",
    "o6 = net6.compute_activation(ReLU)\n",
    "o7 = net7.compute_activation(ReLU)\n",
    "o8 = net8.compute_activation(ReLU)\n",
    "output_l2 = o4.concat([o5, o6, o7, o8])\n",
    "\n",
    "# Layer 3\n",
    "input_l3 = output_l2.add_x0()\n",
    "wh9_l3 = wh9 * input_l3\n",
    "wh10_l3 = wh10 * input_l3\n",
    "wh11_l3 = wh11 * input_l3\n",
    "wh12_l3 = wh12 * input_l3\n",
    "net9 = wh9_l3.sum()\n",
    "net10 = wh10_l3.sum()\n",
    "net11 = wh11_l3.sum()\n",
    "net12 = wh12_l3.sum()\n",
    "o9 = net9.compute_activation(ReLU)\n",
    "o10 = net10.compute_activation(ReLU)\n",
    "o11 = net11.compute_activation(ReLU)\n",
    "o12 = net12.compute_activation(ReLU)\n",
    "output_l3 = o9.concat([o10, o11, o12])\n",
    "loss = output_l3.compute_loss(y, MeanSquaredError)\n",
    "\n",
    "\n",
    "# Backpropagation\n",
    "loss.backward()\n",
    "\n",
    "print(\"============ Layer 1 ============\")\n",
    "print(wh1_x)\n",
    "print(wh2_x)\n",
    "print(wh3_x)\n",
    "print(net1)\n",
    "print(net2)\n",
    "print(net3)\n",
    "print(o1)\n",
    "print(o2)\n",
    "print(o3)\n",
    "print(output_l1)\n",
    "print(\"\\n============ Layer 2 ============\")\n",
    "print(input_l2)\n",
    "print(wh4_l2)\n",
    "print(wh5_l2)\n",
    "print(wh6_l2)\n",
    "print(wh7_l2)\n",
    "print(wh8_l2)\n",
    "print(net4)\n",
    "print(net5)\n",
    "print(net6)\n",
    "print(net7)\n",
    "print(net8)\n",
    "print(o4)\n",
    "print(o5)\n",
    "print(o6)\n",
    "print(o7)\n",
    "print(o8)\n",
    "print(output_l2)\n",
    "print(\"\\n============ Layer 3 ============\")\n",
    "print(input_l3)\n",
    "print(wh9_l3)\n",
    "print(wh10_l3)\n",
    "print(wh11_l3)\n",
    "print(wh12_l3)\n",
    "print(net9)\n",
    "print(net10)\n",
    "print(net11)\n",
    "print(net12)\n",
    "print(o9)\n",
    "print(o10)\n",
    "print(o11)\n",
    "print(o12)\n",
    "print(output_l3)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Layer Test Case**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "layer = Dense(5, \"linear\", \"glorot_uniform\", 7)\n",
    "print(layer.weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
