{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import src.tensor\n",
    "\n",
    "from src.tensor import Tensor\n",
    "from src.activation_function import Linear, ReLU, Sigmoid\n",
    "from src.loss_function import MeanSquaredError\n",
    "from src.layer import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tensor Test Case**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value: [1. 2.], Gradient: [0. 0.], Op: None\n",
      "Value: [3. 4.], Gradient: [0. 0.], Op: None\n",
      "Value: [4. 6.], Gradient: [0. 0.], Op: +\n",
      "Value: [-2. -2.], Gradient: [0. 0.], Op: +\n",
      "Value: [3. 8.], Gradient: [0. 0.], Op: *\n",
      "Value: [0.33333333 0.5       ], Gradient: [0. 0.], Op: *\n"
     ]
    }
   ],
   "source": [
    "# Basic operations\n",
    "a = Tensor(np.array([1,2]))\n",
    "b = Tensor(np.array([3,4]))\n",
    "c = a + b\n",
    "d = a - b\n",
    "e = a * b\n",
    "f = a / b\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "print(d)\n",
    "print(e)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value: [1. 2.], Gradient: [0. 0.], Op: None\n",
      "Value: [1. 2.], Gradient: [0. 0.], Op: Linear\n",
      "Value: [1. 2.], Gradient: [0. 0.], Op: ReLU\n",
      "Value: [4.], Gradient: [0.], Op: MeanSquaredError\n",
      "Value: [4.], Gradient: [0.], Op: MeanSquaredError\n"
     ]
    }
   ],
   "source": [
    "# Activation function and loss function\n",
    "a = Tensor(np.array([1,2]))\n",
    "b = a.compute_activation(Linear)\n",
    "c = a.compute_activation(ReLU)\n",
    "d = b.compute_loss(Tensor(np.array([3,4])), MeanSquaredError)\n",
    "e = c.compute_loss(Tensor(np.array([3,4])), MeanSquaredError)\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "print(d)\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Before backpropagation ----------\n",
      "Value: [1. 2.], Gradient: [0. 0.], Op: None\n",
      "Value: [3. 4.], Gradient: [0. 0.], Op: None\n",
      "Value: [4. 6.], Gradient: [0. 0.], Op: +\n",
      "Value: [-2. -2.], Gradient: [0. 0.], Op: +\n",
      "Value: [ -8. -12.], Gradient: [0. 0.], Op: *\n",
      "Value: [ -8. -12.], Gradient: [0. 0.], Op: Linear\n",
      "Value: [0. 0.], Gradient: [0. 0.], Op: ReLU\n",
      "Value: [125.], Gradient: [0.], Op: MeanSquaredError\n",
      "Value: [1.], Gradient: [0.], Op: MeanSquaredError\n",
      "\n",
      "---------- After backpropagation ----------\n",
      "Value: [1. 2.], Gradient: [ -54. -156.], Op: None\n",
      "Value: [3. 4.], Gradient: [54. 78.], Op: None\n",
      "Value: [4. 6.], Gradient: [36. 52.], Op: +\n",
      "Value: [-2. -2.], Gradient: [ -72. -156.], Op: +\n",
      "Value: [ -8. -12.], Gradient: [ -9. -13.], Op: *\n",
      "Value: [ -8. -12.], Gradient: [ -9. -13.], Op: Linear\n",
      "Value: [0. 0.], Gradient: [-1. -1.], Op: ReLU\n",
      "Value: [125.], Gradient: [1.], Op: MeanSquaredError\n",
      "Value: [1.], Gradient: [1.], Op: MeanSquaredError\n"
     ]
    }
   ],
   "source": [
    "# Automatic differentiation\n",
    "a = Tensor(np.array([1,2]))\n",
    "b = Tensor(np.array([3,4]))\n",
    "c = a + b\n",
    "d = a - b\n",
    "e = c * d\n",
    "f = e.compute_activation(Linear)\n",
    "g = e.compute_activation(ReLU)\n",
    "h = f.compute_loss(np.array([1,1]), MeanSquaredError)\n",
    "i = g.compute_loss(np.array([1,1]), MeanSquaredError)\n",
    "\n",
    "print(\"---------- Before backpropagation ----------\")\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "print(d)\n",
    "print(e)\n",
    "print(f)\n",
    "print(g)\n",
    "print(h)\n",
    "print(i)\n",
    "\n",
    "h.backward()\n",
    "i.backward()\n",
    "\n",
    "print(\"\\n---------- After backpropagation ----------\")\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "print(d)\n",
    "print(e)\n",
    "print(f)\n",
    "print(g)\n",
    "print(h)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Before backpropagation ----------\n",
      "Value: [2. 3. 4.], Gradient: [0. 0. 0.], Op: None\n",
      "Value: [3. 4. 5.], Gradient: [0. 0. 0.], Op: None\n",
      "Value: [ 2.  6. 12.], Gradient: [0. 0. 0.], Op: *\n",
      "Value: [ 3.  8. 15.], Gradient: [0. 0. 0.], Op: *\n",
      "Value: [20.], Gradient: [0.], Op: sum\n",
      "Value: [26.], Gradient: [0.], Op: sum\n",
      "Value: [20.], Gradient: [0.], Op: ReLU\n",
      "Value: [26.], Gradient: [0.], Op: ReLU\n",
      "Value: [20. 26.], Gradient: [0. 0.], Op: concat\n",
      "Value: [80.], Gradient: [0.], Op: MeanSquaredError\n",
      "\n",
      "---------- After backpropagation ----------\n",
      "Value: [2. 3. 4.], Gradient: [ 4.  8. 12.], Op: None\n",
      "Value: [3. 4. 5.], Gradient: [12. 24. 36.], Op: None\n",
      "Value: [ 2.  6. 12.], Gradient: [4. 4. 4.], Op: *\n",
      "Value: [ 3.  8. 15.], Gradient: [12. 12. 12.], Op: *\n",
      "Value: [20.], Gradient: [4.], Op: sum\n",
      "Value: [26.], Gradient: [12.], Op: sum\n",
      "Value: [20.], Gradient: [4.], Op: ReLU\n",
      "Value: [26.], Gradient: [12.], Op: ReLU\n",
      "Value: [20. 26.], Gradient: [ 4. 12.], Op: concat\n",
      "Value: [80.], Gradient: [1.], Op: MeanSquaredError\n"
     ]
    }
   ],
   "source": [
    "## Simulation of one layer with two neurons (h1 and h2)\n",
    "\n",
    "# Initial values\n",
    "x = Tensor(np.array([1, 2, 3]), tensor_type=\"input\")            # input, x[0] is always 1\n",
    "wh1 = Tensor(np.array([2, 3, 4]), tensor_type=\"weight\")         # weights of neuron h1, wh1[0] = b1 (bias)\n",
    "wh2 = Tensor(np.array([3, 4, 5]), tensor_type=\"weight\")         # weights of neuron h2, wh2[0] = b2 (bias)\n",
    "\n",
    "# Calculate net\n",
    "wh1_x = wh1 * x\n",
    "wh2_x = wh2 * x\n",
    "net1 = wh1_x.sum()\n",
    "net2 = wh2_x.sum()\n",
    "\n",
    "# Calculate output\n",
    "o1 = net1.compute_activation(ReLU)\n",
    "o2 = net2.compute_activation(ReLU)\n",
    "\n",
    "# Calculate loss\n",
    "output = o1.concat([o2])\n",
    "loss = output.compute_loss(np.array([16, 14]), MeanSquaredError)\n",
    "\n",
    "print(\"---------- Before backpropagation ----------\")\n",
    "print(wh1)\n",
    "print(wh2)\n",
    "print(wh1_x)\n",
    "print(wh2_x)\n",
    "print(net1)\n",
    "print(net2)\n",
    "print(o1)\n",
    "print(o2)\n",
    "print(output)\n",
    "print(loss)\n",
    "\n",
    "# Do automated differentiation\n",
    "loss.backward()\n",
    "\n",
    "print(\"\\n---------- After backpropagation ----------\")\n",
    "print(wh1)\n",
    "print(wh2)\n",
    "print(wh1_x)\n",
    "print(wh2_x)\n",
    "print(net1)\n",
    "print(net2)\n",
    "print(o1)\n",
    "print(o2)\n",
    "print(output)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Layer Test Case**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer = Dense(5, \"linear\", \"glorot_uniform\", 7)\n",
    "# print(layer.weight)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
